## 전승아 회고: 
- 배운 점
    - 트랜스포머 모델의 구조(인코더-디코더, 멀티헤드 어텐션, 포지셔널 인코딩)를 실제로 구현하며 원리를 깊이 이해할 수 있었음
    - 한국어 자연어 처리에서 형태소 분석기 대신 SubwordTextEncoder를 활용하는 방법과 그 장단점 파악

- 아쉬운 점
    - 데이터셋의 크기와 다양성 부족으로 챗봇의 대화 성능에 한계가 있음
    - TFLite 변환 과정에서 복잡한 트랜스포머 모델의 최적화 어려움

- 느낀 점
    - 트랜스포머 모델이 시퀀스 데이터 처리에 얼마나 강력한지 체감했음
    - 실제 구현과 이론 간의 간극을 메우는 과정에서 많은 학습 발생
    - 케창딥에서 봤던 내용을 실습으로 정리. 

- 어려웠던 점
    - 자잘한 오류 해결, 한국어 형태소 변환의 문제. 

## 이하은 회고: 
- 배운 점 : 
    - 트랜스포머 전반적인 구조 및 인코더, 디코더 부분이 어떠한 역할을 하는지를 알게되었고, 챗봇을 만들기 위해 플러우를 알게되었다. 특히 한국어 대회 데이터셋으로 학습 진행 시 데이터의 전처리 부분이 영문으로 학습할 때의 전처리와 조금 다르다는 것을 배웠다.
- 아쉬운 점 : 
    - 트랜스포머 모델이 이전에 다뤘던 모델보다 복잡하다보니 모든 코드를 이해하고 넘어갈 수 없었다. 코드를 읽는 실력 열심히 키워야겠다.
- 느낀 점 : 
    - 스마트한 챗봇을 만드는 것이 진짜 쉬운일이 아니고 GPT와 같은 다양한 기능을 수행할 수 있는 LLM 모델은 또한 얼마나 복잡한 코드로 구성되어 있는지 상상만해도 눈앞이 캄캄하다...
- 여려웠던 점 : 
    - 중간 중간 발생하는 에러들을 바로 잡는데 코드 전체가 너무 길기도 하고 복잡하기도 해서 쉽게쉽게 해결되지 않았다. 기존에 있는 노드 9번 코드를 재활용 했으면 조금 더 수월했을 지도 모른다. 잘 모르는 부분에서는 조금조금씩 알아가는 것이 좋고 급발진하다가 길을 잃게 된다.
